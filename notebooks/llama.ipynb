{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- RMS Normalization\n",
    "- Rotary Positional Embeddings\n",
    "- KV-Cache\n",
    "- Multi-Query Attention\n",
    "- Grouped Multi-Query Attention\n",
    "- SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/LLaMA-Architecture.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = -1\n",
    "    hidden_size: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_attention_heads: int = 32 # number of attention heads for queries\n",
    "    n_key_value_heads: Optional[int] = None # number of attention heads for keys and values\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    intermediate_size: int = 16384\n",
    "    norm_eps: float = 1e-5\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.eps = config.norm_eps\n",
    "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(config.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embeddings: Combining Absolute and Relative\n",
    "\n",
    "- Introduction\n",
    "  - Discusses the importance of positional embeddings in Transformer models.\n",
    "  \n",
    "- Absolute Positional Embeddings\n",
    "  - Explains how absolute positional embeddings work.\n",
    "  - Highlights limitations like fixed sequence length and lack of relative context.\n",
    "  \n",
    "- Relative Positional Embeddings\n",
    "  - Introduces the concept of relative positional embeddings.\n",
    "  - Discusses the computational challenges and inefficiencies.\n",
    "  \n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "  - Combines the advantages of both absolute and relative embeddings.\n",
    "  - Uses rotation to encode position, preserving relative distances.\n",
    "  \n",
    "- Matrix Formulation\n",
    "  - Explains the mathematical formulation behind RoPE.\n",
    "  \n",
    "- Implementation\n",
    "  - Shows how RoPE can be implemented efficiently in PyTorch.\n",
    "  \n",
    "- Experiments and Conclusion\n",
    "  - Shares results of experiments showing RoPE's effectiveness and efficiency compared to other methods.\n",
    "\n",
    "The video provides a comprehensive overview of Rotary Positional Embeddings, a new method that combines the strengths of both absolute and relative positional embeddings. It delves into the mathematical details and practical implementation, concluding with experimental results that validate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 10000.0\n",
    "max_seq_len = 10\n",
    "hidden_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_i = 1000^{(-2i) / hidden_size} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theta_i = 1000 ^ (-2(i-1) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(max_seq_len)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas = torch.outer(m, thetas).float() #[max_seq_len, hidden_size//2]\n",
    "m_thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.000,     0.000,     0.000,     0.000,     0.000,     0.000,     0.000],\n",
       "        [    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000],\n",
       "        [    2.000,     0.537,     0.144,     0.039,     0.010,     0.003,     0.001],\n",
       "        [    3.000,     0.805,     0.216,     0.058,     0.016,     0.004,     0.001],\n",
       "        [    4.000,     1.073,     0.288,     0.077,     0.021,     0.006,     0.001],\n",
       "        [    5.000,     1.341,     0.360,     0.097,     0.026,     0.007,     0.002],\n",
       "        [    6.000,     1.610,     0.432,     0.116,     0.031,     0.008,     0.002],\n",
       "        [    7.000,     1.878,     0.504,     0.135,     0.036,     0.010,     0.003],\n",
       "        [    8.000,     2.146,     0.576,     0.154,     0.041,     0.011,     0.003],\n",
       "        [    9.000,     2.414,     0.648,     0.174,     0.047,     0.013,     0.003]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex = torch.polar(torch.ones_like(m_thetas), m_thetas)\n",
    "freqs_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j],\n",
       "        [ 0.540+0.841j,  0.964+0.265j,  0.997+0.072j,  1.000+0.019j,  1.000+0.005j,  1.000+0.001j,  1.000+0.000j],\n",
       "        [-0.416+0.909j,  0.859+0.511j,  0.990+0.143j,  0.999+0.039j,  1.000+0.010j,  1.000+0.003j,  1.000+0.001j],\n",
       "        [-0.990+0.141j,  0.693+0.721j,  0.977+0.214j,  0.998+0.058j,  1.000+0.016j,  1.000+0.004j,  1.000+0.001j],\n",
       "        [-0.654-0.757j,  0.477+0.879j,  0.959+0.284j,  0.997+0.077j,  1.000+0.021j,  1.000+0.006j,  1.000+0.001j],\n",
       "        [ 0.284-0.959j,  0.227+0.974j,  0.936+0.352j,  0.995+0.096j,  1.000+0.026j,  1.000+0.007j,  1.000+0.002j],\n",
       "        [ 0.960-0.279j, -0.039+0.999j,  0.908+0.419j,  0.993+0.116j,  1.000+0.031j,  1.000+0.008j,  1.000+0.002j],\n",
       "        [ 0.754+0.657j, -0.302+0.953j,  0.876+0.483j,  0.991+0.135j,  0.999+0.036j,  1.000+0.010j,  1.000+0.003j],\n",
       "        [-0.146+0.989j, -0.544+0.839j,  0.839+0.544j,  0.988+0.154j,  0.999+0.041j,  1.000+0.011j,  1.000+0.003j],\n",
       "        [-0.911+0.412j, -0.747+0.665j,  0.797+0.603j,  0.985+0.173j,  0.999+0.047j,  1.000+0.013j,  1.000+0.003j]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.829,  0.845,  0.287,  0.847,  1.093, -1.112, -1.014, -0.143, -0.983, -1.131,  1.792,  0.058,  0.219,  0.337]]]),\n",
       " torch.Size([1, 1, 14]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, hidden_size)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.829,  0.845],\n",
       "          [ 0.287,  0.847],\n",
       "          [ 1.093, -1.112],\n",
       "          [-1.014, -0.143],\n",
       "          [-0.983, -1.131],\n",
       "          [ 1.792,  0.058],\n",
       "          [ 0.219,  0.337]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.float().view(*x.shape[:-1], -1, 2)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.829+0.845j,  0.287+0.847j,  1.093-1.112j, -1.014-0.143j, -0.983-1.131j,  1.792+0.058j,  0.219+0.337j]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex = torch.view_as_complex(x_reshaped)\n",
    "x_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.vocab_size != -1, 'vocab_size must be specified'\n",
    "\n",
    "        self.device = config.device\n",
    "        \n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size) \n",
    "        self.freq_complex = self._precompute_pos_frequencies()\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.rms_norm = RMSNorm(config.hidden_size, eps=config.norm_eps)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def precompute_pos_frequencies(self, config: LlamaConfig) -> torch.Tensor:\n",
    "        \"\"\"Precompute positional frequencies for sinusoidal positional embeddings.\"\"\"\n",
    "        \n",
    "        theta = 10000.0\n",
    "        hidden_size = config.hidden_size\n",
    "        max_seq_len = config.max_seq_len\n",
    "        device = config.device\n",
    "        \n",
    "        assert hidden_size % 2 == 0, 'hidden_size must be even: RoPe cannot be appied to odd-dimensional embeddings'\n",
    "        \n",
    "        # theta_i = 1000 ^ (-2(i) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "        thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "        m = torch.arange(max_seq_len, device=device, dtype=torch.float) # (max_seq_len, )\n",
    "        freqs = torch.outer(m, theta) # (max_seq_len, hidden_size/2)\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs).to(device) # (max_seq_len, hidden_size/2)\n",
    "        return freqs_complex\n",
    "\n",
    "    def apply_rotary_embeddings(self, x: torch.Tensor, position: int) -> torch.Tensor:\n",
    "        x_complex = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2)) # (B,seq_len,h,head_dim) -> (B,seq_len,h,head_dim//2)\n",
    "        freq_complex = self.freq_complex[:, position][None, :, None, ...] # (seq_len, head_dim//2) -> (1, seq_len, 1, head_dim//2)\n",
    "        x_rotated = x_complex * freq_complex # (B,seq_len,h,head_dim//2)\n",
    "        x_real = torch.view_as_complex(x_rotated) # (B,seq_len,h,head_dim//2) -> (B, seq_len, h, head_dim//2, 2)\n",
    "        return x_real.view(x.shape).type_as(x).to(self.device)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, start_position: int, target: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        bs, seq_len = input_ids.shape\n",
    "        assert seq_len == 1, 'sequence length must be 1'\n",
    "        \n",
    "        token_embeddings = self.embeddings(input_ids) # (batch_size, seq_len, hidden_size): (bs, 1, 4096)\n",
    "        freq_complex = self.freq_complex[start_position:start_position + seq_len] # (batch_size, hidden_size)\n",
    "        \n",
    "        for layer in self.llama_blocks:\n",
    "            token_embeddings = layer(token_embeddings, start_position, freq_complex)\n",
    "        logits = self.head(self.rms_norm(token_embeddings))\n",
    "        \n",
    "        if target is None:\n",
    "            return {'logits': logits}\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            return {'logits': logits, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
