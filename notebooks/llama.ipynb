{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=160)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- RMS Normalization\n",
    "- Rotary Positional Embeddings\n",
    "- KV-Cache\n",
    "- Multi-Query Attention\n",
    "- Grouped Multi-Query Attention\n",
    "- SwiGLU Activation Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/LLaMA-Architecture.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = -1\n",
    "    hidden_size: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_query_heads: int = 32 # number of attention heads for queries\n",
    "    n_kv_heads: Optional[int] = None # number of attention heads for keys and values\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    intermediate_size: int = 16384\n",
    "    norm_eps: float = 1e-5\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "    device: str = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embeddings: Combining Absolute and Relative\n",
    "\n",
    "- Introduction\n",
    "  - Discusses the importance of positional embeddings in Transformer models.\n",
    "  \n",
    "- Absolute Positional Embeddings\n",
    "  - Explains how absolute positional embeddings work.\n",
    "  - Highlights limitations like fixed sequence length and lack of relative context.\n",
    "  \n",
    "- Relative Positional Embeddings\n",
    "  - Introduces the concept of relative positional embeddings.\n",
    "  - Discusses the computational challenges and inefficiencies.\n",
    "  \n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "  - Combines the advantages of both absolute and relative embeddings.\n",
    "  - Uses rotation to encode position, preserving relative distances.\n",
    "  \n",
    "- Matrix Formulation\n",
    "  - Explains the mathematical formulation behind RoPE.\n",
    "  \n",
    "- Implementation\n",
    "  - Shows how RoPE can be implemented efficiently in PyTorch.\n",
    "  \n",
    "- Experiments and Conclusion\n",
    "  - Shares results of experiments showing RoPE's effectiveness and efficiency compared to other methods.\n",
    "\n",
    "The video provides a comprehensive overview of Rotary Positional Embeddings, a new method that combines the strengths of both absolute and relative positional embeddings. It delves into the mathematical details and practical implementation, concluding with experimental results that validate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 10000.0\n",
    "max_seq_len = 10\n",
    "hidden_size = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_i = 1000^{(-2i) / hidden_size} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theta_i = 1000 ^ (-2(i-1) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(max_seq_len)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas = torch.outer(m, thetas).float() #[max_seq_len, hidden_size//2]\n",
    "m_thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.000,     0.000,     0.000,     0.000,     0.000,     0.000,     0.000],\n",
       "        [    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000],\n",
       "        [    2.000,     0.537,     0.144,     0.039,     0.010,     0.003,     0.001],\n",
       "        [    3.000,     0.805,     0.216,     0.058,     0.016,     0.004,     0.001],\n",
       "        [    4.000,     1.073,     0.288,     0.077,     0.021,     0.006,     0.001],\n",
       "        [    5.000,     1.341,     0.360,     0.097,     0.026,     0.007,     0.002],\n",
       "        [    6.000,     1.610,     0.432,     0.116,     0.031,     0.008,     0.002],\n",
       "        [    7.000,     1.878,     0.504,     0.135,     0.036,     0.010,     0.003],\n",
       "        [    8.000,     2.146,     0.576,     0.154,     0.041,     0.011,     0.003],\n",
       "        [    9.000,     2.414,     0.648,     0.174,     0.047,     0.013,     0.003]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex = torch.polar(torch.ones_like(m_thetas), m_thetas)\n",
    "freqs_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j],\n",
       "        [ 0.540+0.841j,  0.964+0.265j,  0.997+0.072j,  1.000+0.019j,  1.000+0.005j,  1.000+0.001j,  1.000+0.000j],\n",
       "        [-0.416+0.909j,  0.859+0.511j,  0.990+0.143j,  0.999+0.039j,  1.000+0.010j,  1.000+0.003j,  1.000+0.001j],\n",
       "        [-0.990+0.141j,  0.693+0.721j,  0.977+0.214j,  0.998+0.058j,  1.000+0.016j,  1.000+0.004j,  1.000+0.001j],\n",
       "        [-0.654-0.757j,  0.477+0.879j,  0.959+0.284j,  0.997+0.077j,  1.000+0.021j,  1.000+0.006j,  1.000+0.001j],\n",
       "        [ 0.284-0.959j,  0.227+0.974j,  0.936+0.352j,  0.995+0.096j,  1.000+0.026j,  1.000+0.007j,  1.000+0.002j],\n",
       "        [ 0.960-0.279j, -0.039+0.999j,  0.908+0.419j,  0.993+0.116j,  1.000+0.031j,  1.000+0.008j,  1.000+0.002j],\n",
       "        [ 0.754+0.657j, -0.302+0.953j,  0.876+0.483j,  0.991+0.135j,  0.999+0.036j,  1.000+0.010j,  1.000+0.003j],\n",
       "        [-0.146+0.989j, -0.544+0.839j,  0.839+0.544j,  0.988+0.154j,  0.999+0.041j,  1.000+0.011j,  1.000+0.003j],\n",
       "        [-0.911+0.412j, -0.747+0.665j,  0.797+0.603j,  0.985+0.173j,  0.999+0.047j,  1.000+0.013j,  1.000+0.003j]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.130, -0.038, -0.408,  0.565,  0.456,  2.403, -1.228,  0.225, -0.635,  0.797,  2.449, -0.316,  0.735, -1.491]]]),\n",
       " torch.Size([1, 1, 14]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, hidden_size)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.130, -0.038],\n",
       "          [-0.408,  0.565],\n",
       "          [ 0.456,  2.403],\n",
       "          [-1.228,  0.225],\n",
       "          [-0.635,  0.797],\n",
       "          [ 2.449, -0.316],\n",
       "          [ 0.735, -1.491]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.float().view(*x.shape[:-1], -1, 2)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.130-0.038j, -0.408+0.565j,  0.456+2.403j, -1.228+0.225j, -0.635+0.797j,  2.449-0.316j,  0.735-1.491j]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex = torch.view_as_complex(x_reshaped)\n",
    "x_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.540+0.841j, 0.964+0.265j, 0.997+0.072j, 1.000+0.019j, 1.000+0.005j, 1.000+0.001j, 1.000+0.000j]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 1\n",
    "freq_complex = freqs_complex[position:position+1][None, :, None, ...] # (seq_len, head_dim//2) -> (1, seq_len, 1, head_dim//2)\n",
    "freq_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rotated = x_complex * freq_complex # (B,seq_len,h,head_dim//2)\n",
    "x_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.578-0.971j, -0.543+0.436j,  0.282+2.430j, -1.233+0.201j, -0.640+0.793j,  2.450-0.312j,  0.736-1.491j]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-0.578, -0.971],\n",
       "           [-0.543,  0.436],\n",
       "           [ 0.282,  2.430],\n",
       "           [-1.233,  0.201],\n",
       "           [-0.640,  0.793],\n",
       "           [ 2.450, -0.312],\n",
       "           [ 0.736, -1.491]]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real = torch.view_as_real(x_rotated) # (B,seq_len,h,head_dim//2) -> (B, seq_len, h, head_dim//2, 2)\n",
    "x_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.578, -0.971, -0.543,  0.436,  0.282,  2.430, -1.233,  0.201, -0.640,  0.793,  2.450, -0.312,  0.736, -1.491]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = x_real.view(x.shape).type_as(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 14]), torch.Size([1, 1, 14]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Layer Normalization (RMSNorm) emerges as a refinement over traditional normalization techniques employed in transformer architectures, aiming to address computational overheads while retaining, or even enhancing, the model's performance.  \n",
    "Initially, transformers relied on BatchNormalization, as introduced in Vaswani et al. 2017, but the advent of LLaMA brought RMSNorm to the forefront, offering a more streamlined normalization approach.  \n",
    "\n",
    "The core idea behind RMSNorm is derived from its predecessor, Layer Normalization (LayerNorm), which was noted for its two pivotal properties: re-centering and re-scaling. Re-centering aids in rendering the model robust to shift noises in inputs and weights,  \n",
    "while re-scaling preserves the output representations amidst random scaling of inputs and weights. However, it was recognized that the majority of the benefits stemmed from the re-scaling aspect, which led to the conceptualization of RMSNorm.  \n",
    "\n",
    "RMSNorm, embodying simplicity, omits the re-centering (mean-centering) operation from LayerNorm, focusing solely on the re-scaling invariance. This is achieved by normalizing the summed inputs according to the root mean square (RMS) statistic, expressed as:  \n",
    "$$ \\overline{a}_i = \\frac{a_i}{\\texttt{RMS}}$$ where $$ \\texttt{RMS} = \\sqrt{ \\frac{1}{n}\\sum_{i=1}^n a_i^2} $$  \n",
    "Here, $a_i$ denotes the activation of the ith neuron. By sidestepping the mean computation, RMSNorm not only simplifies the normalization process but also curtails the computational time, marking a significant stride towards efficiency.   \n",
    "\n",
    "This nuanced approach dovetails with the transition from post-normalization to pre-normalization, as observed in LLaMA. Unlike the original Transformer that applied normalization post the attention layer, LLaMA, inspired by GPT-3,   \n",
    "normalizes the input before feeding it to the self-attention and feed-forward layers. RMSNorm serves as the linchpin in this pre-normalization process, ensuring that the neural network's activations are duly regulated before entering the subsequent layers.  \n",
    "\n",
    "The impact of RMSNorm is palpable, with empirical evidence indicating a reduction in running time between 7% to 64% when juxtaposed with LayerNorm, all the while maintaining comparable performance levels. This demonstrates RMSNorm's capability to balance computational   \n",
    "efficiency with performance efficacy, making it a valuable asset in the ongoing evolution of transformer architectures.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/RMS-Normalization.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        \"\"\"\n",
    "        Root Mean Square Layer Normalization\n",
    "\n",
    "        Args:\n",
    "            config: LlamaConfig\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.eps = config.norm_eps\n",
    "        self.register_parameter('scale', nn.Parameter(torch.ones(config.hidden_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, seq_len, hidden_size)\n",
    "        x_normed = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) # (B,seq_len,hidden_size) * (B,seq_len,1) -> (B,seq_len,hidden_size)\n",
    "        return (self.scale * x_normed).type_as(x) # (hidden_size) * (B,seq_len,hidden_size) -> (B,seq_len,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 14]), torch.Size([14]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 1, hidden_size)\n",
    "weight = torch.rand(hidden_size)\n",
    "\n",
    "x.shape, weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.504, -1.463, -0.425, -0.583, -1.264,  0.704, -1.060,  2.345, -0.085, -0.532,  1.153, -0.391,  0.668,  0.391]],\n",
       "\n",
       "        [[ 0.074, -1.664,  0.687, -1.206, -0.661,  0.020,  0.506,  0.861, -1.237, -1.634, -1.176, -0.485,  1.366, -0.425]],\n",
       "\n",
       "        [[-1.669, -0.200, -1.376, -0.486, -1.456,  0.302,  0.564,  0.285,  1.166,  0.259,  1.690, -0.332, -0.048,  1.428]],\n",
       "\n",
       "        [[-0.935,  0.575, -1.455,  0.106,  0.166, -0.861, -0.134,  0.637, -2.042, -0.377, -0.890, -1.621,  1.226, -0.494]],\n",
       "\n",
       "        [[ 0.552,  1.407,  0.389,  0.889,  0.699,  0.043, -1.603, -0.388,  0.041,  2.053, -0.263, -1.263, -1.061,  0.747]],\n",
       "\n",
       "        [[-1.165,  0.980,  0.225, -0.881, -1.379,  0.693,  0.401,  0.031, -2.200,  1.415, -0.688, -0.124,  0.505,  0.850]],\n",
       "\n",
       "        [[-2.132,  1.294, -0.105, -0.667, -0.562, -1.021,  1.387,  1.551, -0.330, -0.462,  0.878,  0.426, -0.602, -0.029]],\n",
       "\n",
       "        [[-2.502, -0.479, -1.740, -0.187, -0.360,  1.217,  1.033,  0.414,  0.240,  0.057, -0.475, -0.484,  0.986,  0.322]],\n",
       "\n",
       "        [[ 0.444,  1.921,  0.278, -1.021,  1.277, -0.346, -0.310, -0.431, -1.003, -0.323,  1.007, -0.441,  1.982,  0.845]],\n",
       "\n",
       "        [[ 0.452, -0.669,  1.599,  2.045,  1.265, -0.707,  0.653,  0.251, -0.924, -0.329, -0.443, -0.029,  0.328, -1.659]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normed = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.089, -1.454, -0.040, -0.459, -0.276,  0.388, -1.046,  1.616, -0.071, -0.154,  0.481, -0.164,  0.502,  0.062]],\n",
       "\n",
       "        [[ 0.013, -1.653,  0.065, -0.949, -0.144,  0.011,  0.500,  0.593, -1.041, -0.474, -0.490, -0.204,  1.026, -0.067]],\n",
       "\n",
       "        [[-0.295, -0.199, -0.131, -0.383, -0.318,  0.166,  0.556,  0.197,  0.981,  0.075,  0.705, -0.139, -0.036,  0.225]],\n",
       "\n",
       "        [[-0.165,  0.571, -0.138,  0.084,  0.036, -0.474, -0.132,  0.439, -1.718, -0.109, -0.371, -0.680,  0.921, -0.078]],\n",
       "\n",
       "        [[ 0.098,  1.398,  0.037,  0.700,  0.153,  0.024, -1.582, -0.267,  0.035,  0.595, -0.110, -0.530, -0.797,  0.118]],\n",
       "\n",
       "        [[-0.206,  0.974,  0.021, -0.694, -0.302,  0.382,  0.395,  0.022, -1.851,  0.410, -0.287, -0.052,  0.379,  0.134]],\n",
       "\n",
       "        [[-0.377,  1.286, -0.010, -0.525, -0.123, -0.563,  1.368,  1.069, -0.278, -0.134,  0.366,  0.179, -0.452, -0.005]],\n",
       "\n",
       "        [[-0.442, -0.476, -0.166, -0.147, -0.079,  0.670,  1.020,  0.285,  0.202,  0.017, -0.198, -0.203,  0.741,  0.051]],\n",
       "\n",
       "        [[ 0.078,  1.909,  0.026, -0.804,  0.279, -0.191, -0.306, -0.297, -0.844, -0.094,  0.420, -0.185,  1.489,  0.133]],\n",
       "\n",
       "        [[ 0.080, -0.665,  0.152,  1.610,  0.277, -0.389,  0.644,  0.173, -0.778, -0.095, -0.185, -0.012,  0.247, -0.262]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = weight * x_normed\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEAttentoinHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGQAttention(nn.Module):\n",
    "        \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Notation:\n",
    "        #   b - batch size\n",
    "        #   n - sequence length\n",
    "        #   h - number of heads\n",
    "        #   d - embedding dimension\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.n_kv_heads = config.n_kv_heads or config.n_query_heads\n",
    "        self.n_query_heads = config.n_query_heads\n",
    "        self.n_replicas = self.n_kv_heads // self.n_query_heads\n",
    "        self.head_dim = self.hidden_size // self.n_query_heads\n",
    "        \n",
    "        self.wq = nn.Linear(self.hidden_size, self.n_query_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(self.hidden_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(self.hidden_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.cache_key = torch.zeros((config.max_batch_size, config.max_seq_len, self.n_kv_heads, self.head_dim), device=config.device)\n",
    "        self.cache_value = torch.zeros((config.max_batch_size, config.max_seq_len, self.n_kv_heads, self.head_dim), device=config.device)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, position: int, freqs_complex: torch.Tensor) -> torch.Tensor:\n",
    "        bs, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.wq(x) # (bs, seq_len, hidden_size) @ (hidden_size, n_query_heads) --> (bs, seq_len, n_query_heads)\n",
    "        k = self.wk(x) # (bs, seq_len, hidden_size) @ (hidden_size, n_kv_heads) --> (bs, seq_len, n_kv_heads)\n",
    "        v = self.wv(x) # (bs, seq_len, hidden_size) @ (hidden_size, n_kv_heads) --> (bs, seq_len, n_kv_heads)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        q = q.view(bs, seq_len, self.n_query_heads, self.head_dim)\n",
    "        k = k.view(bs, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        v = v.view(bs, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        \n",
    "        q_rotated = self.apply_rotary_embeddings(q, position, freqs_complex)\n",
    "        k_rotated = self.apply_rotary_embeddings(k, position, freqs_complex)\n",
    "        \n",
    "        self.cache_key[:bs, position:position+seq_len] = k_rotated\n",
    "        self.cache_value[:bs, position:position+seq_len] = v\n",
    "        \n",
    "        cache_key = self.cache_key[:bs, :position+seq_len]\n",
    "        cache_value = self.cache_value[:bs, :position+seq_len]\n",
    "        \n",
    "        rep_cache_key = self.repeat_kv(cache_key, self.n_replicas)\n",
    "        rep_cache_value = self.repeat_kv(cache_value, self.n_replicas)\n",
    "        \n",
    "        attn_scores = (q_rotated @ rep_cache_key.transpose(-1, -2)) / self.head_dim ** 0.5\n",
    "        attn_scores = attn_scores.softmax(dim=-1)\n",
    "        weighted_values = attn_scores @ rep_cache_value\n",
    "        \n",
    "        weighted_values = weighted_values.contignous().view(bs, seq_len, -1)\n",
    "        return self.wo(weighted_values)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def apply_rotary_embeddings(self, x: torch.Tensor, position: int, freq_complex: torch.Tensor) -> torch.Tensor:\n",
    "        x_complex = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2)) # (B,seq_len,h,head_dim) -> (B,seq_len,h,head_dim//2)\n",
    "        freq_complex = freq_complex[position:position+1][None, :, None, ...] # (seq_len, head_dim//2) -> (1, seq_len, 1, head_dim//2)\n",
    "        x_rotated = x_complex * freq_complex # (B,seq_len,h,head_dim//2)\n",
    "        x_real = torch.view_as_complex(x_rotated) # (B,seq_len,h,head_dim//2) -> (B, seq_len, h, head_dim//2, 2)\n",
    "        return x_real.view(x.shape).type_as(x).to(self.device)\n",
    "    \n",
    "    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "        in_shape = x.shape\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        return (x[:, :, None, :] # (bs, seq_len, n_kv_heads, head_dim) -> (bs, seq_len, n_kv_heads, 1, head_dim)\n",
    "                .expand(-1, -1, -1, n_rep, -1)\n",
    "                .reshape(in_shape)) \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SwiGLU activation function is utilized as a replacement for the ReLU non-linearity, Google’s PaLM and Meta’s LLaMA use it to improve the performance of the FFN (position-wise feed-forward network) layers in the Transformer architecture. SwiGLU is based on the Swish activation function and its formulation is given by:\n",
    "\n",
    "$$\n",
    "\\text{SwiGLU}(x) = \\text{Swish}\\beta (xW + b) \\otimes (xV + c)\n",
    "$$\n",
    "\n",
    "where $ \\beta $ is a learnable parameter, and $ \\otimes $ denotes element-wise multiplication. The Swish function, denoted by $ \\text{Swish}\\beta(x) $, is defined as $ x \\sigma(\\beta x) $, where $ \\sigma $ is the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(\\beta x)\n",
    "$$\n",
    "\n",
    "and $ \\sigma $ is the sigmoid function defined by:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def gelu(x):\n",
    "   return x * norm.cdf(x)\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(0, x)\n",
    "\n",
    "def swish(x, beta=1):\n",
    "   return x * (1 / (1 + np.exp(-beta * x)))\n",
    "\n",
    "x_values = np.linspace(-5, 5, 500)\n",
    "gelu_values = gelu(x_values)\n",
    "relu_values = relu(x_values)\n",
    "swish_values = swish(x_values)\n",
    "\n",
    "plt.plot(x_values, gelu_values, label='GELU')\n",
    "plt.plot(x_values, relu_values, label='ReLU')\n",
    "plt.plot(x_values, swish_values, label='Swish')\n",
    "plt.title(\"GELU, ReLU, and Swish Activation Functions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular models, the dimensionality of certain layers is adjusted when employing the SwiGLU activation function. The cited adjustment is from $ 4d $ to $ \\frac{2}{3} 4d $, as opposed to the dimensionality used in a model referred to as PaLM【19†source】【20†source】【21†source】【22†source】. This adjustment is in an effort to maintain a constant number of parameters and computational load, despite the use of different activation functions or variants of the Transformer's feed-forward networks (FFN) layers【15†source】.\n",
    "\n",
    "Specifically, in the context of Transformer models, the adjustment in dimensionality accompanies the replacement of ReLU with the SwiGLU activation function in the feed-forward sublayers. This replacement and dimensionality adjustment is part of efforts to explore variants of the Gated Linear Units (GLU) in Transformer models to potentially enhance performance.\n",
    "\n",
    "The SwiGLU activation function and the dimensionality adjustment are part of a broader discussion on enhancing the performance of Transformer models through the exploration of different activation functions and their impact on the model architecture and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "        hidden_dim = 4 * config.hidden_size\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if config.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(config.ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = config.multiple_of * ((hidden_dim + config.multiple_of - 1) // config.multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(config.hidden_size, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, config.hidden_size, bias=False)\n",
    "        self.w3 = nn.Linear(config.hidden_size, hidden_dim, bias=False)\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.n_heads = config.n_query_heads\n",
    "        self.head_dim = self.hidden_size // self.n_heads # 4096 // 32 = 128\n",
    "        \n",
    "        self.attention = SelfAttention (config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.attention_norm = RMSNorm(config)\n",
    "        self.ffn_norm = RMSNorm(config)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, position: int, freq_complex: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, seq_len, hidden_size)\n",
    "        x = x + self.attention(self.attention_norm(x), position, freq_complex) # (B, seq_len, hidden_size)\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.vocab_size != -1, 'vocab_size must be specified'\n",
    "\n",
    "        self.device = config.device\n",
    "        \n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size) \n",
    "        self.freq_complex = self._precompute_pos_frequencies(config=config)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.rms_norm = RMSNorm(config)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def _precompute_pos_frequencies(self, config: LlamaConfig) -> torch.Tensor:\n",
    "        \"\"\"Precompute positional frequencies for sinusoidal positional embeddings.\"\"\"\n",
    "        \n",
    "        theta = 10000.0\n",
    "        hidden_size = config.hidden_size\n",
    "        max_seq_len = config.max_seq_len * 2\n",
    "        device = config.device\n",
    "        \n",
    "        assert hidden_size % 2 == 0, 'hidden_size must be even: RoPe cannot be appied to odd-dimensional embeddings'\n",
    "        \n",
    "        # theta_i = 1000 ^ (-2(i) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "        thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "        m = torch.arange(max_seq_len, dtype=torch.float) # (max_seq_len, )\n",
    "        freqs = torch.outer(m, thetas) # (max_seq_len, hidden_size/2)\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs).to(device) # (max_seq_len, hidden_size/2)\n",
    "        return freqs_complex    \n",
    "    \n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, start_position: int, target: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        bs, seq_len = input_ids.shape\n",
    "        assert seq_len == 1, 'sequence length must be 1'\n",
    "        \n",
    "        token_embeddings = self.embeddings(input_ids) # (batch_size, seq_len, hidden_size): (bs, 1, 4096)\n",
    "        freq_complex = self.freq_complex[start_position:start_position + seq_len] # (batch_size, hidden_size)\n",
    "        \n",
    "        for layer in self.llama_blocks:\n",
    "            token_embeddings = layer(token_embeddings, start_position, freq_complex)\n",
    "        logits = self.head(self.rms_norm(token_embeddings))\n",
    "        \n",
    "        if target is None:\n",
    "            return {'logits': logits}\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            return {'logits': logits, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLaMA(LlamaConfig(vocab_size=32000, device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model: LLaMA) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        config_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        seed: int = 42,\n",
    "        device: str = None\n",
    "        ) -> 'LLaMAModel':\n",
    "        \n",
    "        if device is None:\n",
    "            device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f'>>> Using device: {device}')\n",
    "        \n",
    "        start = time.time()\n",
    "        model_path = list(Path(ckpt_dir).glob('*.pth'))\n",
    "        assert len(model_path) > 0, 'No model checkpoint provided!'\n",
    "        ckpt_path = model_path[0]\n",
    "        print(f'>>> Loading checkpoint from {ckpt_path}')\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        print(f'>>> Checkpoint loaded in {time.time() - start:.2f} seconds') \n",
    "        \n",
    "        with open(config_path, 'rb') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        config: LlamaConfig = LlamaConfig(\n",
    "            max_batch_size=max_batch_size,\n",
    "            max_seq_len=max_seq_len,\n",
    "            device=device,\n",
    "            **config\n",
    "        )\n",
    "        print(f'>>> Using config: {config}')\n",
    "        \n",
    "        start = time.time()\n",
    "        tokenizer = SentencePieceProcessor()\n",
    "        tokenizer.load(tokenizer_path)\n",
    "        config.vocab_size = tokenizer.vocab_size()\n",
    "        print(f'>>> Tokenizer loaded in {time.time() - start:.2f} seconds')\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        elif device == 'mps':\n",
    "            torch.set_default_tensor_type(torch.FloatTensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
    "            \n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        start = time.time()\n",
    "        model = LLaMA(config)\n",
    "        del checkpoint['rope.freqs']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "        print(f'>>> Model loaded in {time.time() - start:.2f} seconds')\n",
    "        \n",
    "        return {'model': model, 'tokenizer': tokenizer, 'config': config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list((Path('../llama-2-7b/').glob('*.pth'))))\n",
    "# llama_dict = LLaMAModel.build(\n",
    "#     ckpt_dir='../llama-2-7b/',\n",
    "#     tokenizer_path='../tokenizer.model',\n",
    "#     config_path='../llama-2-7b/config.json',\n",
    "#     max_seq_len=1024,\n",
    "#     max_batch_size=3,\n",
    "# )\n",
    "# print('All ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../llama-2-7b/consolidated.00.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in checkpoint.keys(): print(k)\n",
    "# for name, param in model.state_dict().items():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted['embeddings.weight'] = checkpoint['tok_embeddings.weight']\n",
    "converted['rms_norm.scale'] = checkpoint['norm.weight']\n",
    "converted['head.weight'] = checkpoint['output.weight']\n",
    "converted['llama_blocks.llama_0.attention.wq.weight'] = checkpoint['layers.0.attention.wq.weight']\n",
    "converted['llama_blocks.llama_0.attention.wk.weight'] = checkpoint['layers.0.attention.wk.weight']\n",
    "converted['llama_blocks.llama_0.attention.wv.weight'] = checkpoint['layers.0.attention.wv.weight']\n",
    "converted['llama_blocks.llama_0.attention.wo.weight'] = checkpoint['layers.0.attention.wo.weight']\n",
    "converted['llama_blocks.llama_0.ffn.w1.weight'] = checkpoint['layers.0.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_0.ffn.w2.weight'] = checkpoint['layers.0.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_0.ffn.w3.weight'] = checkpoint['layers.0.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_0.attention_norm.scale'] = checkpoint['layers.0.attention_norm.weight']\n",
    "converted['llama_blocks.llama_0.ffn_norm.scale'] = checkpoint['layers.0.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_1.attention.wq.weight'] = checkpoint['layers.1.attention.wq.weight']\n",
    "converted['llama_blocks.llama_1.attention.wk.weight'] = checkpoint['layers.1.attention.wk.weight']\n",
    "converted['llama_blocks.llama_1.attention.wv.weight'] = checkpoint['layers.1.attention.wv.weight']\n",
    "converted['llama_blocks.llama_1.attention.wo.weight'] = checkpoint['layers.1.attention.wo.weight']\n",
    "converted['llama_blocks.llama_1.ffn.w1.weight'] = checkpoint['layers.1.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_1.ffn.w2.weight'] = checkpoint['layers.1.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_1.ffn.w3.weight'] = checkpoint['layers.1.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_1.attention_norm.scale'] = checkpoint['layers.1.attention_norm.weight']\n",
    "converted['llama_blocks.llama_1.ffn_norm.scale'] = checkpoint['layers.1.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_2.attention.wq.weight'] = checkpoint['layers.2.attention.wq.weight']\n",
    "converted['llama_blocks.llama_2.attention.wk.weight'] = checkpoint['layers.2.attention.wk.weight']\n",
    "converted['llama_blocks.llama_2.attention.wv.weight'] = checkpoint['layers.2.attention.wv.weight']\n",
    "converted['llama_blocks.llama_2.attention.wo.weight'] = checkpoint['layers.2.attention.wo.weight']\n",
    "converted['llama_blocks.llama_2.ffn.w1.weight'] = checkpoint['layers.2.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_2.ffn.w2.weight'] = checkpoint['layers.2.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_2.ffn.w3.weight'] = checkpoint['layers.2.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_2.attention_norm.scale'] = checkpoint['layers.2.attention_norm.weight']\n",
    "converted['llama_blocks.llama_2.ffn_norm.scale'] = checkpoint['layers.2.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_3.attention.wq.weight'] = checkpoint['layers.3.attention.wq.weight']\n",
    "converted['llama_blocks.llama_3.attention.wk.weight'] = checkpoint['layers.3.attention.wk.weight']\n",
    "converted['llama_blocks.llama_3.attention.wv.weight'] = checkpoint['layers.3.attention.wv.weight']\n",
    "converted['llama_blocks.llama_3.attention.wo.weight'] = checkpoint['layers.3.attention.wo.weight']\n",
    "converted['llama_blocks.llama_3.ffn.w1.weight'] = checkpoint['layers.3.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_3.ffn.w2.weight'] = checkpoint['layers.3.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_3.ffn.w3.weight'] = checkpoint['layers.3.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_3.attention_norm.scale'] = checkpoint['layers.3.attention_norm.weight']\n",
    "converted['llama_blocks.llama_3.ffn_norm.scale'] = checkpoint['layers.3.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_4.attention.wq.weight'] = checkpoint['layers.4.attention.wq.weight']\n",
    "converted['llama_blocks.llama_4.attention.wk.weight'] = checkpoint['layers.4.attention.wk.weight']\n",
    "converted['llama_blocks.llama_4.attention.wv.weight'] = checkpoint['layers.4.attention.wv.weight']\n",
    "converted['llama_blocks.llama_4.attention.wo.weight'] = checkpoint['layers.4.attention.wo.weight']\n",
    "converted['llama_blocks.llama_4.ffn.w1.weight'] = checkpoint['layers.4.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_4.ffn.w2.weight'] = checkpoint['layers.4.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_4.ffn.w3.weight'] = checkpoint['layers.4.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_4.attention_norm.scale'] = checkpoint['layers.4.attention_norm.weight']\n",
    "converted['llama_blocks.llama_4.ffn_norm.scale'] = checkpoint['layers.4.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_5.attention.wq.weight'] = checkpoint['layers.5.attention.wq.weight']\n",
    "converted['llama_blocks.llama_5.attention.wk.weight'] = checkpoint['layers.5.attention.wk.weight']\n",
    "converted['llama_blocks.llama_5.attention.wv.weight'] = checkpoint['layers.5.attention.wv.weight']\n",
    "converted['llama_blocks.llama_5.attention.wo.weight'] = checkpoint['layers.5.attention.wo.weight']\n",
    "converted['llama_blocks.llama_5.ffn.w1.weight'] = checkpoint['layers.5.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_5.ffn.w2.weight'] = checkpoint['layers.5.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_5.ffn.w3.weight'] = checkpoint['layers.5.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_5.attention_norm.scale'] = checkpoint['layers.5.attention_norm.weight']\n",
    "converted['llama_blocks.llama_5.ffn_norm.scale'] = checkpoint['layers.5.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_6.attention.wq.weight'] = checkpoint['layers.6.attention.wq.weight']\n",
    "converted['llama_blocks.llama_6.attention.wk.weight'] = checkpoint['layers.6.attention.wk.weight']\n",
    "converted['llama_blocks.llama_6.attention.wv.weight'] = checkpoint['layers.6.attention.wv.weight']\n",
    "converted['llama_blocks.llama_6.attention.wo.weight'] = checkpoint['layers.6.attention.wo.weight']\n",
    "converted['llama_blocks.llama_6.ffn.w1.weight'] = checkpoint['layers.6.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_6.ffn.w2.weight'] = checkpoint['layers.6.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_6.ffn.w3.weight'] = checkpoint['layers.6.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_6.attention_norm.scale'] = checkpoint['layers.6.attention_norm.weight']\n",
    "converted['llama_blocks.llama_6.ffn_norm.scale'] = checkpoint['layers.6.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_7.attention.wq.weight'] = checkpoint['layers.7.attention.wq.weight']\n",
    "converted['llama_blocks.llama_7.attention.wk.weight'] = checkpoint['layers.7.attention.wk.weight']\n",
    "converted['llama_blocks.llama_7.attention.wv.weight'] = checkpoint['layers.7.attention.wv.weight']\n",
    "converted['llama_blocks.llama_7.attention.wo.weight'] = checkpoint['layers.7.attention.wo.weight']\n",
    "converted['llama_blocks.llama_7.ffn.w1.weight'] = checkpoint['layers.7.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_7.ffn.w2.weight'] = checkpoint['layers.7.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_7.ffn.w3.weight'] = checkpoint['layers.7.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_7.attention_norm.scale'] = checkpoint['layers.7.attention_norm.weight']\n",
    "converted['llama_blocks.llama_7.ffn_norm.scale'] = checkpoint['layers.7.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_8.attention.wq.weight'] = checkpoint['layers.8.attention.wq.weight']\n",
    "converted['llama_blocks.llama_8.attention.wk.weight'] = checkpoint['layers.8.attention.wk.weight']\n",
    "converted['llama_blocks.llama_8.attention.wv.weight'] = checkpoint['layers.8.attention.wv.weight']\n",
    "converted['llama_blocks.llama_8.attention.wo.weight'] = checkpoint['layers.8.attention.wo.weight']\n",
    "converted['llama_blocks.llama_8.ffn.w1.weight'] = checkpoint['layers.8.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_8.ffn.w2.weight'] = checkpoint['layers.8.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_8.ffn.w3.weight'] = checkpoint['layers.8.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_8.attention_norm.scale'] = checkpoint['layers.8.attention_norm.weight']\n",
    "converted['llama_blocks.llama_8.ffn_norm.scale'] = checkpoint['layers.8.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_9.attention.wq.weight'] = checkpoint['layers.9.attention.wq.weight']\n",
    "converted['llama_blocks.llama_9.attention.wk.weight'] = checkpoint['layers.9.attention.wk.weight']\n",
    "converted['llama_blocks.llama_9.attention.wv.weight'] = checkpoint['layers.9.attention.wv.weight']\n",
    "converted['llama_blocks.llama_9.attention.wo.weight'] = checkpoint['layers.9.attention.wo.weight']\n",
    "converted['llama_blocks.llama_9.ffn.w1.weight'] = checkpoint['layers.9.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_9.ffn.w2.weight'] = checkpoint['layers.9.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_9.ffn.w3.weight'] = checkpoint['layers.9.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_9.attention_norm.scale'] = checkpoint['layers.9.attention_norm.weight']\n",
    "converted['llama_blocks.llama_9.ffn_norm.scale'] = checkpoint['layers.9.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_10.attention.wq.weight'] = checkpoint['layers.10.attention.wq.weight']\n",
    "converted['llama_blocks.llama_10.attention.wk.weight'] = checkpoint['layers.10.attention.wk.weight']\n",
    "converted['llama_blocks.llama_10.attention.wv.weight'] = checkpoint['layers.10.attention.wv.weight']\n",
    "converted['llama_blocks.llama_10.attention.wo.weight'] = checkpoint['layers.10.attention.wo.weight']\n",
    "converted['llama_blocks.llama_10.ffn.w1.weight'] = checkpoint['layers.10.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_10.ffn.w2.weight'] = checkpoint['layers.10.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_10.ffn.w3.weight'] = checkpoint['layers.10.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_10.attention_norm.scale'] = checkpoint['layers.10.attention_norm.weight']\n",
    "converted['llama_blocks.llama_10.ffn_norm.scale'] = checkpoint['layers.10.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_11.attention.wq.weight'] = checkpoint['layers.11.attention.wq.weight']\n",
    "converted['llama_blocks.llama_11.attention.wk.weight'] = checkpoint['layers.11.attention.wk.weight']\n",
    "converted['llama_blocks.llama_11.attention.wv.weight'] = checkpoint['layers.11.attention.wv.weight']\n",
    "converted['llama_blocks.llama_11.attention.wo.weight'] = checkpoint['layers.11.attention.wo.weight']\n",
    "converted['llama_blocks.llama_11.ffn.w1.weight'] = checkpoint['layers.11.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_11.ffn.w2.weight'] = checkpoint['layers.11.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_11.ffn.w3.weight'] = checkpoint['layers.11.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_11.attention_norm.scale'] = checkpoint['layers.11.attention_norm.weight']\n",
    "converted['llama_blocks.llama_11.ffn_norm.scale'] = checkpoint['layers.11.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_12.attention.wq.weight'] = checkpoint['layers.12.attention.wq.weight']\n",
    "converted['llama_blocks.llama_12.attention.wk.weight'] = checkpoint['layers.12.attention.wk.weight']\n",
    "converted['llama_blocks.llama_12.attention.wv.weight'] = checkpoint['layers.12.attention.wv.weight']\n",
    "converted['llama_blocks.llama_12.attention.wo.weight'] = checkpoint['layers.12.attention.wo.weight']\n",
    "converted['llama_blocks.llama_12.ffn.w1.weight'] = checkpoint['layers.12.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_12.ffn.w2.weight'] = checkpoint['layers.12.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_12.ffn.w3.weight'] = checkpoint['layers.12.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_12.attention_norm.scale'] = checkpoint['layers.12.attention_norm.weight']\n",
    "converted['llama_blocks.llama_12.ffn_norm.scale'] = checkpoint['layers.12.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_13.attention.wq.weight'] = checkpoint['layers.13.attention.wq.weight']\n",
    "converted['llama_blocks.llama_13.attention.wk.weight'] = checkpoint['layers.13.attention.wk.weight']\n",
    "converted['llama_blocks.llama_13.attention.wv.weight'] = checkpoint['layers.13.attention.wv.weight']\n",
    "converted['llama_blocks.llama_13.attention.wo.weight'] = checkpoint['layers.13.attention.wo.weight']\n",
    "converted['llama_blocks.llama_13.ffn.w1.weight'] = checkpoint['layers.13.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_13.ffn.w2.weight'] = checkpoint['layers.13.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_13.ffn.w3.weight'] = checkpoint['layers.13.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_13.attention_norm.scale'] = checkpoint['layers.13.attention_norm.weight']\n",
    "converted['llama_blocks.llama_13.ffn_norm.scale'] = checkpoint['layers.13.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_14.attention.wq.weight'] = checkpoint['layers.14.attention.wq.weight']\n",
    "converted['llama_blocks.llama_14.attention.wk.weight'] = checkpoint['layers.14.attention.wk.weight']\n",
    "converted['llama_blocks.llama_14.attention.wv.weight'] = checkpoint['layers.14.attention.wv.weight']\n",
    "converted['llama_blocks.llama_14.attention.wo.weight'] = checkpoint['layers.14.attention.wo.weight']\n",
    "converted['llama_blocks.llama_14.ffn.w1.weight'] = checkpoint['layers.14.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_14.ffn.w2.weight'] = checkpoint['layers.14.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_14.ffn.w3.weight'] = checkpoint['layers.14.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_14.attention_norm.scale'] = checkpoint['layers.14.attention_norm.weight']\n",
    "converted['llama_blocks.llama_14.ffn_norm.scale'] = checkpoint['layers.14.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_15.attention.wq.weight'] = checkpoint['layers.15.attention.wq.weight']\n",
    "converted['llama_blocks.llama_15.attention.wk.weight'] = checkpoint['layers.15.attention.wk.weight']\n",
    "converted['llama_blocks.llama_15.attention.wv.weight'] = checkpoint['layers.15.attention.wv.weight']\n",
    "converted['llama_blocks.llama_15.attention.wo.weight'] = checkpoint['layers.15.attention.wo.weight']\n",
    "converted['llama_blocks.llama_15.ffn.w1.weight'] = checkpoint['layers.15.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_15.ffn.w2.weight'] = checkpoint['layers.15.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_15.ffn.w3.weight'] = checkpoint['layers.15.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_15.attention_norm.scale'] = checkpoint['layers.15.attention_norm.weight']\n",
    "converted['llama_blocks.llama_15.ffn_norm.scale'] = checkpoint['layers.15.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_16.attention.wq.weight'] = checkpoint['layers.16.attention.wq.weight']\n",
    "converted['llama_blocks.llama_16.attention.wk.weight'] = checkpoint['layers.16.attention.wk.weight']\n",
    "converted['llama_blocks.llama_16.attention.wv.weight'] = checkpoint['layers.16.attention.wv.weight']\n",
    "converted['llama_blocks.llama_16.attention.wo.weight'] = checkpoint['layers.16.attention.wo.weight']\n",
    "converted['llama_blocks.llama_16.ffn.w1.weight'] = checkpoint['layers.16.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_16.ffn.w2.weight'] = checkpoint['layers.16.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_16.ffn.w3.weight'] = checkpoint['layers.16.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_16.attention_norm.scale'] = checkpoint['layers.16.attention_norm.weight']\n",
    "converted['llama_blocks.llama_16.ffn_norm.scale'] = checkpoint['layers.16.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_17.attention.wq.weight'] = checkpoint['layers.17.attention.wq.weight']\n",
    "converted['llama_blocks.llama_17.attention.wk.weight'] = checkpoint['layers.17.attention.wk.weight']\n",
    "converted['llama_blocks.llama_17.attention.wv.weight'] = checkpoint['layers.17.attention.wv.weight']\n",
    "converted['llama_blocks.llama_17.attention.wo.weight'] = checkpoint['layers.17.attention.wo.weight']\n",
    "converted['llama_blocks.llama_17.ffn.w1.weight'] = checkpoint['layers.17.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_17.ffn.w2.weight'] = checkpoint['layers.17.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_17.ffn.w3.weight'] = checkpoint['layers.17.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_17.attention_norm.scale'] = checkpoint['layers.17.attention_norm.weight']\n",
    "converted['llama_blocks.llama_17.ffn_norm.scale'] = checkpoint['layers.17.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_18.attention.wq.weight'] = checkpoint['layers.18.attention.wq.weight']\n",
    "converted['llama_blocks.llama_18.attention.wk.weight'] = checkpoint['layers.18.attention.wk.weight']\n",
    "converted['llama_blocks.llama_18.attention.wv.weight'] = checkpoint['layers.18.attention.wv.weight']\n",
    "converted['llama_blocks.llama_18.attention.wo.weight'] = checkpoint['layers.18.attention.wo.weight']\n",
    "converted['llama_blocks.llama_18.ffn.w1.weight'] = checkpoint['layers.18.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_18.ffn.w2.weight'] = checkpoint['layers.18.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_18.ffn.w3.weight'] = checkpoint['layers.18.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_18.attention_norm.scale'] = checkpoint['layers.18.attention_norm.weight']\n",
    "converted['llama_blocks.llama_18.ffn_norm.scale'] = checkpoint['layers.18.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_19.attention.wq.weight'] = checkpoint['layers.19.attention.wq.weight']\n",
    "converted['llama_blocks.llama_19.attention.wk.weight'] = checkpoint['layers.19.attention.wk.weight']\n",
    "converted['llama_blocks.llama_19.attention.wv.weight'] = checkpoint['layers.19.attention.wv.weight']\n",
    "converted['llama_blocks.llama_19.attention.wo.weight'] = checkpoint['layers.19.attention.wo.weight']\n",
    "converted['llama_blocks.llama_19.ffn.w1.weight'] = checkpoint['layers.19.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_19.ffn.w2.weight'] = checkpoint['layers.19.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_19.ffn.w3.weight'] = checkpoint['layers.19.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_19.attention_norm.scale'] = checkpoint['layers.19.attention_norm.weight']\n",
    "converted['llama_blocks.llama_19.ffn_norm.scale'] = checkpoint['layers.19.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_20.attention.wq.weight'] = checkpoint['layers.20.attention.wq.weight']\n",
    "converted['llama_blocks.llama_20.attention.wk.weight'] = checkpoint['layers.20.attention.wk.weight']\n",
    "converted['llama_blocks.llama_20.attention.wv.weight'] = checkpoint['layers.20.attention.wv.weight']\n",
    "converted['llama_blocks.llama_20.attention.wo.weight'] = checkpoint['layers.20.attention.wo.weight']\n",
    "converted['llama_blocks.llama_20.ffn.w1.weight'] = checkpoint['layers.20.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_20.ffn.w2.weight'] = checkpoint['layers.20.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_20.ffn.w3.weight'] = checkpoint['layers.20.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_20.attention_norm.scale'] = checkpoint['layers.20.attention_norm.weight']\n",
    "converted['llama_blocks.llama_20.ffn_norm.scale'] = checkpoint['layers.20.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_21.attention.wq.weight'] = checkpoint['layers.21.attention.wq.weight']\n",
    "converted['llama_blocks.llama_21.attention.wk.weight'] = checkpoint['layers.21.attention.wk.weight']\n",
    "converted['llama_blocks.llama_21.attention.wv.weight'] = checkpoint['layers.21.attention.wv.weight']\n",
    "converted['llama_blocks.llama_21.attention.wo.weight'] = checkpoint['layers.21.attention.wo.weight']\n",
    "converted['llama_blocks.llama_21.ffn.w1.weight'] = checkpoint['layers.21.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_21.ffn.w2.weight'] = checkpoint['layers.21.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_21.ffn.w3.weight'] = checkpoint['layers.21.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_21.attention_norm.scale'] = checkpoint['layers.21.attention_norm.weight']\n",
    "converted['llama_blocks.llama_21.ffn_norm.scale'] = checkpoint['layers.21.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_22.attention.wq.weight'] = checkpoint['layers.22.attention.wq.weight']\n",
    "converted['llama_blocks.llama_22.attention.wk.weight'] = checkpoint['layers.22.attention.wk.weight']\n",
    "converted['llama_blocks.llama_22.attention.wv.weight'] = checkpoint['layers.22.attention.wv.weight']\n",
    "converted['llama_blocks.llama_22.attention.wo.weight'] = checkpoint['layers.22.attention.wo.weight']\n",
    "converted['llama_blocks.llama_22.ffn.w1.weight'] = checkpoint['layers.22.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_22.ffn.w2.weight'] = checkpoint['layers.22.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_22.ffn.w3.weight'] = checkpoint['layers.22.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_22.attention_norm.scale'] = checkpoint['layers.22.attention_norm.weight']\n",
    "converted['llama_blocks.llama_22.ffn_norm.scale'] = checkpoint['layers.22.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_23.attention.wq.weight'] = checkpoint['layers.23.attention.wq.weight']\n",
    "converted['llama_blocks.llama_23.attention.wk.weight'] = checkpoint['layers.23.attention.wk.weight']\n",
    "converted['llama_blocks.llama_23.attention.wv.weight'] = checkpoint['layers.23.attention.wv.weight']\n",
    "converted['llama_blocks.llama_23.attention.wo.weight'] = checkpoint['layers.23.attention.wo.weight']\n",
    "converted['llama_blocks.llama_23.ffn.w1.weight'] = checkpoint['layers.23.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_23.ffn.w2.weight'] = checkpoint['layers.23.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_23.ffn.w3.weight'] = checkpoint['layers.23.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_23.attention_norm.scale'] = checkpoint['layers.23.attention_norm.weight']\n",
    "converted['llama_blocks.llama_23.ffn_norm.scale'] = checkpoint['layers.23.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_24.attention.wq.weight'] = checkpoint['layers.24.attention.wq.weight']\n",
    "converted['llama_blocks.llama_24.attention.wk.weight'] = checkpoint['layers.24.attention.wk.weight']\n",
    "converted['llama_blocks.llama_24.attention.wv.weight'] = checkpoint['layers.24.attention.wv.weight']\n",
    "converted['llama_blocks.llama_24.attention.wo.weight'] = checkpoint['layers.24.attention.wo.weight']\n",
    "converted['llama_blocks.llama_24.ffn.w1.weight'] = checkpoint['layers.24.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_24.ffn.w2.weight'] = checkpoint['layers.24.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_24.ffn.w3.weight'] = checkpoint['layers.24.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_24.attention_norm.scale'] = checkpoint['layers.24.attention_norm.weight']\n",
    "converted['llama_blocks.llama_24.ffn_norm.scale'] = checkpoint['layers.24.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_25.attention.wq.weight'] = checkpoint['layers.25.attention.wq.weight']\n",
    "converted['llama_blocks.llama_25.attention.wk.weight'] = checkpoint['layers.25.attention.wk.weight']\n",
    "converted['llama_blocks.llama_25.attention.wv.weight'] = checkpoint['layers.25.attention.wv.weight']\n",
    "converted['llama_blocks.llama_25.attention.wo.weight'] = checkpoint['layers.25.attention.wo.weight']\n",
    "converted['llama_blocks.llama_25.ffn.w1.weight'] = checkpoint['layers.25.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_25.ffn.w2.weight'] = checkpoint['layers.25.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_25.ffn.w3.weight'] = checkpoint['layers.25.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_25.attention_norm.scale'] = checkpoint['layers.25.attention_norm.weight']\n",
    "converted['llama_blocks.llama_25.ffn_norm.scale'] = checkpoint['layers.25.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_26.attention.wq.weight'] = checkpoint['layers.26.attention.wq.weight']\n",
    "converted['llama_blocks.llama_26.attention.wk.weight'] = checkpoint['layers.26.attention.wk.weight']\n",
    "converted['llama_blocks.llama_26.attention.wv.weight'] = checkpoint['layers.26.attention.wv.weight']\n",
    "converted['llama_blocks.llama_26.attention.wo.weight'] = checkpoint['layers.26.attention.wo.weight']\n",
    "converted['llama_blocks.llama_26.ffn.w1.weight'] = checkpoint['layers.26.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_26.ffn.w2.weight'] = checkpoint['layers.26.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_26.ffn.w3.weight'] = checkpoint['layers.26.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_26.attention_norm.scale'] = checkpoint['layers.26.attention_norm.weight']\n",
    "converted['llama_blocks.llama_26.ffn_norm.scale'] = checkpoint['layers.26.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_27.attention.wq.weight'] = checkpoint['layers.27.attention.wq.weight']\n",
    "converted['llama_blocks.llama_27.attention.wk.weight'] = checkpoint['layers.27.attention.wk.weight']\n",
    "converted['llama_blocks.llama_27.attention.wv.weight'] = checkpoint['layers.27.attention.wv.weight']\n",
    "converted['llama_blocks.llama_27.attention.wo.weight'] = checkpoint['layers.27.attention.wo.weight']\n",
    "converted['llama_blocks.llama_27.ffn.w1.weight'] = checkpoint['layers.27.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_27.ffn.w2.weight'] = checkpoint['layers.27.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_27.ffn.w3.weight'] = checkpoint['layers.27.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_27.attention_norm.scale'] = checkpoint['layers.27.attention_norm.weight']\n",
    "converted['llama_blocks.llama_27.ffn_norm.scale'] = checkpoint['layers.27.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_28.attention.wq.weight'] = checkpoint['layers.28.attention.wq.weight']\n",
    "converted['llama_blocks.llama_28.attention.wk.weight'] = checkpoint['layers.28.attention.wk.weight']\n",
    "converted['llama_blocks.llama_28.attention.wv.weight'] = checkpoint['layers.28.attention.wv.weight']\n",
    "converted['llama_blocks.llama_28.attention.wo.weight'] = checkpoint['layers.28.attention.wo.weight']\n",
    "converted['llama_blocks.llama_28.ffn.w1.weight'] = checkpoint['layers.28.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_28.ffn.w2.weight'] = checkpoint['layers.28.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_28.ffn.w3.weight'] = checkpoint['layers.28.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_28.attention_norm.scale'] = checkpoint['layers.28.attention_norm.weight']\n",
    "converted['llama_blocks.llama_28.ffn_norm.scale'] = checkpoint['layers.28.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_29.attention.wq.weight'] = checkpoint['layers.29.attention.wq.weight']\n",
    "converted['llama_blocks.llama_29.attention.wk.weight'] = checkpoint['layers.29.attention.wk.weight']\n",
    "converted['llama_blocks.llama_29.attention.wv.weight'] = checkpoint['layers.29.attention.wv.weight']\n",
    "converted['llama_blocks.llama_29.attention.wo.weight'] = checkpoint['layers.29.attention.wo.weight']\n",
    "converted['llama_blocks.llama_29.ffn.w1.weight'] = checkpoint['layers.29.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_29.ffn.w2.weight'] = checkpoint['layers.29.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_29.ffn.w3.weight'] = checkpoint['layers.29.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_29.attention_norm.scale'] = checkpoint['layers.29.attention_norm.weight']\n",
    "converted['llama_blocks.llama_29.ffn_norm.scale'] = checkpoint['layers.29.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_30.attention.wq.weight'] = checkpoint['layers.30.attention.wq.weight']\n",
    "converted['llama_blocks.llama_30.attention.wk.weight'] = checkpoint['layers.30.attention.wk.weight']\n",
    "converted['llama_blocks.llama_30.attention.wv.weight'] = checkpoint['layers.30.attention.wv.weight']\n",
    "converted['llama_blocks.llama_30.attention.wo.weight'] = checkpoint['layers.30.attention.wo.weight']\n",
    "converted['llama_blocks.llama_30.ffn.w1.weight'] = checkpoint['layers.30.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_30.ffn.w2.weight'] = checkpoint['layers.30.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_30.ffn.w3.weight'] = checkpoint['layers.30.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_30.attention_norm.scale'] = checkpoint['layers.30.attention_norm.weight']\n",
    "converted['llama_blocks.llama_30.ffn_norm.scale'] = checkpoint['layers.30.ffn_norm.weight']\n",
    "converted['llama_blocks.llama_31.attention.wq.weight'] = checkpoint['layers.31.attention.wq.weight']\n",
    "converted['llama_blocks.llama_31.attention.wk.weight'] = checkpoint['layers.31.attention.wk.weight']\n",
    "converted['llama_blocks.llama_31.attention.wv.weight'] = checkpoint['layers.31.attention.wv.weight']\n",
    "converted['llama_blocks.llama_31.attention.wo.weight'] = checkpoint['layers.31.attention.wo.weight']\n",
    "converted['llama_blocks.llama_31.ffn.w1.weight'] = checkpoint['layers.31.feed_forward.w1.weight']\n",
    "converted['llama_blocks.llama_31.ffn.w2.weight'] = checkpoint['layers.31.feed_forward.w2.weight']\n",
    "converted['llama_blocks.llama_31.ffn.w3.weight'] = checkpoint['layers.31.feed_forward.w3.weight']\n",
    "converted['llama_blocks.llama_31.attention_norm.scale'] = checkpoint['layers.31.attention_norm.weight']\n",
    "converted['llama_blocks.llama_31.ffn_norm.scale'] = checkpoint['layers.31.ffn_norm.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
