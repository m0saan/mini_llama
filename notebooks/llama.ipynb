{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- RMS Normalization\n",
    "- Rotary Positional Embeddings\n",
    "- KV-Cache\n",
    "- Multi-Query Attention\n",
    "- Grouped Multi-Query Attention\n",
    "- SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/LLaMA-Architecture.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = -1\n",
    "    hidden_size: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_attention_heads: int = 32 # number of attention heads for queries\n",
    "    n_key_value_heads: Optional[int] = None # number of attention heads for keys and values\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    intermediate_size: int = 16384\n",
    "    norm_eps: float = 1e-5\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "    device: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.eps = config.norm_eps\n",
    "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(config.hidden_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embeddings: Combining Absolute and Relative\n",
    "\n",
    "- Introduction\n",
    "  - Discusses the importance of positional embeddings in Transformer models.\n",
    "  \n",
    "- Absolute Positional Embeddings\n",
    "  - Explains how absolute positional embeddings work.\n",
    "  - Highlights limitations like fixed sequence length and lack of relative context.\n",
    "  \n",
    "- Relative Positional Embeddings\n",
    "  - Introduces the concept of relative positional embeddings.\n",
    "  - Discusses the computational challenges and inefficiencies.\n",
    "  \n",
    "- Rotary Positional Embeddings (RoPE)\n",
    "  - Combines the advantages of both absolute and relative embeddings.\n",
    "  - Uses rotation to encode position, preserving relative distances.\n",
    "  \n",
    "- Matrix Formulation\n",
    "  - Explains the mathematical formulation behind RoPE.\n",
    "  \n",
    "- Implementation\n",
    "  - Shows how RoPE can be implemented efficiently in PyTorch.\n",
    "  \n",
    "- Experiments and Conclusion\n",
    "  - Shares results of experiments showing RoPE's effectiveness and efficiency compared to other methods.\n",
    "\n",
    "The video provides a comprehensive overview of Rotary Positional Embeddings, a new method that combines the strengths of both absolute and relative positional embeddings. It delves into the mathematical details and practical implementation, concluding with experimental results that validate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 10000.0\n",
    "max_seq_len = 10\n",
    "hidden_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_i = 1000^{(-2i) / hidden_size} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# theta_i = 1000 ^ (-2(i-1) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(max_seq_len)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas = torch.outer(m, thetas).float() #[max_seq_len, hidden_size//2]\n",
    "m_thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.000,     0.000,     0.000,     0.000,     0.000,     0.000,     0.000],\n",
       "        [    1.000,     0.268,     0.072,     0.019,     0.005,     0.001,     0.000],\n",
       "        [    2.000,     0.537,     0.144,     0.039,     0.010,     0.003,     0.001],\n",
       "        [    3.000,     0.805,     0.216,     0.058,     0.016,     0.004,     0.001],\n",
       "        [    4.000,     1.073,     0.288,     0.077,     0.021,     0.006,     0.001],\n",
       "        [    5.000,     1.341,     0.360,     0.097,     0.026,     0.007,     0.002],\n",
       "        [    6.000,     1.610,     0.432,     0.116,     0.031,     0.008,     0.002],\n",
       "        [    7.000,     1.878,     0.504,     0.135,     0.036,     0.010,     0.003],\n",
       "        [    8.000,     2.146,     0.576,     0.154,     0.041,     0.011,     0.003],\n",
       "        [    9.000,     2.414,     0.648,     0.174,     0.047,     0.013,     0.003]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex = torch.polar(torch.ones_like(m_thetas), m_thetas)\n",
    "freqs_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j,  1.000+0.000j],\n",
       "        [ 0.540+0.841j,  0.964+0.265j,  0.997+0.072j,  1.000+0.019j,  1.000+0.005j,  1.000+0.001j,  1.000+0.000j],\n",
       "        [-0.416+0.909j,  0.859+0.511j,  0.990+0.143j,  0.999+0.039j,  1.000+0.010j,  1.000+0.003j,  1.000+0.001j],\n",
       "        [-0.990+0.141j,  0.693+0.721j,  0.977+0.214j,  0.998+0.058j,  1.000+0.016j,  1.000+0.004j,  1.000+0.001j],\n",
       "        [-0.654-0.757j,  0.477+0.879j,  0.959+0.284j,  0.997+0.077j,  1.000+0.021j,  1.000+0.006j,  1.000+0.001j],\n",
       "        [ 0.284-0.959j,  0.227+0.974j,  0.936+0.352j,  0.995+0.096j,  1.000+0.026j,  1.000+0.007j,  1.000+0.002j],\n",
       "        [ 0.960-0.279j, -0.039+0.999j,  0.908+0.419j,  0.993+0.116j,  1.000+0.031j,  1.000+0.008j,  1.000+0.002j],\n",
       "        [ 0.754+0.657j, -0.302+0.953j,  0.876+0.483j,  0.991+0.135j,  0.999+0.036j,  1.000+0.010j,  1.000+0.003j],\n",
       "        [-0.146+0.989j, -0.544+0.839j,  0.839+0.544j,  0.988+0.154j,  0.999+0.041j,  1.000+0.011j,  1.000+0.003j],\n",
       "        [-0.911+0.412j, -0.747+0.665j,  0.797+0.603j,  0.985+0.173j,  0.999+0.047j,  1.000+0.013j,  1.000+0.003j]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.829,  0.845,  0.287,  0.847,  1.093, -1.112, -1.014, -0.143, -0.983, -1.131,  1.792,  0.058,  0.219,  0.337]]]),\n",
       " torch.Size([1, 1, 14]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, hidden_size)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.829,  0.845],\n",
       "          [ 0.287,  0.847],\n",
       "          [ 1.093, -1.112],\n",
       "          [-1.014, -0.143],\n",
       "          [-0.983, -1.131],\n",
       "          [ 1.792,  0.058],\n",
       "          [ 0.219,  0.337]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.float().view(*x.shape[:-1], -1, 2)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.829+0.845j,  0.287+0.847j,  1.093-1.112j, -1.014-0.143j, -0.983-1.131j,  1.792+0.058j,  0.219+0.337j]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex = torch.view_as_complex(x_reshaped)\n",
    "x_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_complex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.540+0.841j, 0.964+0.265j, 0.997+0.072j, 1.000+0.019j, 1.000+0.005j, 1.000+0.001j, 1.000+0.000j]]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = 1\n",
    "freq_complex = freqs_complex[position:position+1][None, :, None, ...] # (seq_len, head_dim//2) -> (1, seq_len, 1, head_dim//2)\n",
    "freq_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 7])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rotated = x_complex * freq_complex # (B,seq_len,h,head_dim//2)\n",
    "x_rotated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.158-0.241j,  0.052+0.893j,  1.170-1.031j, -1.011-0.162j, -0.977-1.136j,  1.792+0.060j,  0.218+0.337j]]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.158, -0.241],\n",
       "           [ 0.052,  0.893],\n",
       "           [ 1.170, -1.031],\n",
       "           [-1.011, -0.162],\n",
       "           [-0.977, -1.136],\n",
       "           [ 1.792,  0.060],\n",
       "           [ 0.218,  0.337]]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real = torch.view_as_real(x_rotated) # (B,seq_len,h,head_dim//2) -> (B, seq_len, h, head_dim//2, 2)\n",
    "x_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.158, -0.241,  0.052,  0.893,  1.170, -1.031, -1.011, -0.162, -0.977, -1.136,  1.792,  0.060,  0.218,  0.337]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = x_real.view(x.shape).type_as(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 14]), torch.Size([1, 1, 14]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Layer Normalization (RMSNorm) emerges as a refinement over traditional normalization techniques employed in transformer architectures, aiming to address computational overheads while retaining, or even enhancing, the model's performance.  \n",
    "Initially, transformers relied on BatchNormalization, as introduced in Vaswani et al. 2017, but the advent of LLaMA brought RMSNorm to the forefront, offering a more streamlined normalization approach.  \n",
    "\n",
    "The core idea behind RMSNorm is derived from its predecessor, Layer Normalization (LayerNorm), which was noted for its two pivotal properties: re-centering and re-scaling. Re-centering aids in rendering the model robust to shift noises in inputs and weights,  \n",
    "while re-scaling preserves the output representations amidst random scaling of inputs and weights. However, it was recognized that the majority of the benefits stemmed from the re-scaling aspect, which led to the conceptualization of RMSNorm.  \n",
    "\n",
    "RMSNorm, embodying simplicity, omits the re-centering (mean-centering) operation from LayerNorm, focusing solely on the re-scaling invariance. This is achieved by normalizing the summed inputs according to the root mean square (RMS) statistic, expressed as:  \n",
    "$$ \\overline{a}_i = \\frac{a_i}{\\texttt{RMS}}$$ where $$ \\texttt{RMS} = \\sqrt{ \\frac{1}{n}\\sum_{i=1}^n a_i^2} $$  \n",
    "Here, $a_i$ denotes the activation of the ith neuron. By sidestepping the mean computation, RMSNorm not only simplifies the normalization process but also curtails the computational time, marking a significant stride towards efficiency.   \n",
    "\n",
    "This nuanced approach dovetails with the transition from post-normalization to pre-normalization, as observed in LLaMA. Unlike the original Transformer that applied normalization post the attention layer, LLaMA, inspired by GPT-3,   \n",
    "normalizes the input before feeding it to the self-attention and feed-forward layers. RMSNorm serves as the linchpin in this pre-normalization process, ensuring that the neural network's activations are duly regulated before entering the subsequent layers.  \n",
    "\n",
    "The impact of RMSNorm is palpable, with empirical evidence indicating a reduction in running time between 7% to 64% when juxtaposed with LayerNorm, all the while maintaining comparable performance levels. This demonstrates RMSNorm's capability to balance computational   \n",
    "efficiency with performance efficacy, making it a valuable asset in the ongoing evolution of transformer architectures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/RMS-Normalization.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        \"\"\"\n",
    "        Root Mean Square Layer Normalization\n",
    "\n",
    "        Args:\n",
    "            config: LlamaConfig\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.eps = config.norm_eps\n",
    "        self.register_parameter('scale', nn.Parameter(torch.ones(config.hidden_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, seq_len, hidden_size)\n",
    "        x_normed = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) # (B,seq_len,hidden_size) * (B,seq_len,1) -> (B,seq_len,hidden_size)\n",
    "        return (self.scale * x_normed).as_type(x) # (hidden_size) * (B,seq_len,hidden_size) -> (B,seq_len,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1, 14]), torch.Size([14]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 1, hidden_size)\n",
    "weight = torch.rand(hidden_size)\n",
    "\n",
    "x.shape, weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.715, -0.063,  1.662,  0.384, -0.274,  0.199, -1.481,  0.306,  0.391, -1.988, -0.609,  1.110,  0.009, -0.184]],\n",
       "\n",
       "        [[ 0.133,  1.512,  0.952, -0.218,  0.354,  1.264,  0.758,  0.051, -1.049, -0.243, -0.041,  0.474,  1.051,  2.439]],\n",
       "\n",
       "        [[ 0.035,  0.812,  0.134,  0.358,  0.978, -0.392, -1.400, -0.069, -1.031, -2.237,  1.066,  0.061, -1.204,  1.207]],\n",
       "\n",
       "        [[ 0.608, -1.350, -2.355,  1.070, -0.426,  0.727, -0.118, -0.769,  1.163, -0.412, -0.699,  0.546,  0.581,  1.074]],\n",
       "\n",
       "        [[ 0.221,  1.581, -0.584, -1.565, -0.122,  1.393, -0.101, -0.473,  0.566,  0.271,  1.273,  1.366,  0.762,  1.418]],\n",
       "\n",
       "        [[ 0.628,  0.176,  0.488, -0.035, -0.174, -1.080, -1.616,  1.695, -1.271, -1.166, -0.094, -0.720,  0.202, -1.763]],\n",
       "\n",
       "        [[ 1.114,  0.312, -0.263, -1.128,  1.112, -0.910, -0.882, -2.272, -0.046,  1.052, -0.366,  0.364, -0.174, -1.381]],\n",
       "\n",
       "        [[-0.022, -1.279, -0.660,  0.648,  1.433, -0.345,  0.363, -0.753,  0.016,  1.520,  1.251, -0.635, -0.564, -2.010]],\n",
       "\n",
       "        [[-0.262,  0.042, -0.127,  0.756, -0.400, -0.335,  1.709,  1.982, -0.659, -1.417,  1.418,  0.302,  0.952,  0.876]],\n",
       "\n",
       "        [[ 1.268, -0.775, -1.714, -0.147, -0.066,  0.151, -1.310,  1.504,  0.729, -0.024,  1.048,  1.440, -1.038, -0.213]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normed = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + 1e-6)\n",
    "x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    -1.495,     -0.041,      1.045,      0.023,     -0.258,      0.182,     -0.246,      0.003,      0.041,     -0.721,     -0.335,      0.231,\n",
       "               0.008,     -0.106]],\n",
       "\n",
       "        [[     0.116,      0.980,      0.599,     -0.013,      0.334,      1.155,      0.126,      0.001,     -0.109,     -0.088,     -0.022,      0.099,\n",
       "               0.940,      1.403]],\n",
       "\n",
       "        [[     0.030,      0.526,      0.084,      0.021,      0.924,     -0.359,     -0.233,     -0.001,     -0.108,     -0.811,      0.586,      0.013,\n",
       "              -1.077,      0.694]],\n",
       "\n",
       "        [[     0.530,     -0.875,     -1.480,      0.063,     -0.402,      0.665,     -0.020,     -0.008,      0.121,     -0.149,     -0.384,      0.114,\n",
       "               0.520,      0.618]],\n",
       "\n",
       "        [[     0.193,      1.025,     -0.367,     -0.093,     -0.115,      1.274,     -0.017,     -0.005,      0.059,      0.098,      0.700,      0.284,\n",
       "               0.682,      0.815]],\n",
       "\n",
       "        [[     0.548,      0.114,      0.307,     -0.002,     -0.164,     -0.988,     -0.268,      0.019,     -0.132,     -0.423,     -0.052,     -0.150,\n",
       "               0.181,     -1.014]],\n",
       "\n",
       "        [[     0.971,      0.203,     -0.165,     -0.067,      1.051,     -0.832,     -0.146,     -0.025,     -0.005,      0.381,     -0.201,      0.076,\n",
       "              -0.155,     -0.794]],\n",
       "\n",
       "        [[    -0.019,     -0.829,     -0.415,      0.038,      1.354,     -0.316,      0.060,     -0.008,      0.002,      0.551,      0.688,     -0.132,\n",
       "              -0.504,     -1.156]],\n",
       "\n",
       "        [[    -0.229,      0.027,     -0.080,      0.045,     -0.378,     -0.306,      0.284,      0.022,     -0.069,     -0.513,      0.779,      0.063,\n",
       "               0.852,      0.504]],\n",
       "\n",
       "        [[     1.105,     -0.502,     -1.078,     -0.009,     -0.063,      0.138,     -0.218,      0.016,      0.076,     -0.009,      0.576,      0.299,\n",
       "              -0.929,     -0.122]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = weight * x_normed\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.vocab_size != -1, 'vocab_size must be specified'\n",
    "\n",
    "        self.device = config.device\n",
    "        \n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size) \n",
    "        self.freq_complex = self._precompute_pos_frequencies()\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.rms_norm = RMSNorm(config.hidden_size, eps=config.norm_eps)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def precompute_pos_frequencies(self, config: LlamaConfig) -> torch.Tensor:\n",
    "        \"\"\"Precompute positional frequencies for sinusoidal positional embeddings.\"\"\"\n",
    "        \n",
    "        theta = 10000.0\n",
    "        hidden_size = config.hidden_size\n",
    "        max_seq_len = config.max_seq_len\n",
    "        device = config.device\n",
    "        \n",
    "        assert hidden_size % 2 == 0, 'hidden_size must be even: RoPe cannot be appied to odd-dimensional embeddings'\n",
    "        \n",
    "        # theta_i = 1000 ^ (-2(i) / hidden_size) for i in [1, 2, ..., hidden_size/2]\n",
    "        thetas = (1 / theta) ** (torch.arange(0, hidden_size, 2) / hidden_size) #[hidden_size//2, ]\n",
    "        m = torch.arange(max_seq_len, device=device, dtype=torch.float) # (max_seq_len, )\n",
    "        freqs = torch.outer(m, theta) # (max_seq_len, hidden_size/2)\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs).to(device) # (max_seq_len, hidden_size/2)\n",
    "        return freqs_complex\n",
    "\n",
    "    def apply_rotary_embeddings(self, x: torch.Tensor, position: int) -> torch.Tensor:\n",
    "        x_complex = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2)) # (B,seq_len,h,head_dim) -> (B,seq_len,h,head_dim//2)\n",
    "        freq_complex = self.freq_complex[position:position+1][None, :, None, ...] # (seq_len, head_dim//2) -> (1, seq_len, 1, head_dim//2)\n",
    "        x_rotated = x_complex * freq_complex # (B,seq_len,h,head_dim//2)\n",
    "        x_real = torch.view_as_complex(x_rotated) # (B,seq_len,h,head_dim//2) -> (B, seq_len, h, head_dim//2, 2)\n",
    "        return x_real.view(x.shape).type_as(x).to(self.device)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, start_position: int, target: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        bs, seq_len = input_ids.shape\n",
    "        assert seq_len == 1, 'sequence length must be 1'\n",
    "        \n",
    "        token_embeddings = self.embeddings(input_ids) # (batch_size, seq_len, hidden_size): (bs, 1, 4096)\n",
    "        freq_complex = self.freq_complex[start_position:start_position + seq_len] # (batch_size, hidden_size)\n",
    "        \n",
    "        for layer in self.llama_blocks:\n",
    "            token_embeddings = layer(token_embeddings, start_position, freq_complex)\n",
    "        logits = self.head(self.rms_norm(token_embeddings))\n",
    "        \n",
    "        if target is None:\n",
    "            return {'logits': logits}\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            return {'logits': logits, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
